{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOm3L94FxVviUkVrs600809",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cmb170230/NLP-Portfolio/blob/main/CS_4395_WordNet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TqfCgNp-D7Ms",
        "outputId": "ae4a4cd6-d526-4ac5-cfea-1fce22f8fb75"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.8/dist-packages (3.7)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.8/dist-packages (from nltk) (7.1.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.8/dist-packages (from nltk) (2022.6.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.8/dist-packages (from nltk) (1.2.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from nltk) (4.64.1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "!pip install nltk\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##WordNet, Briefly\n",
        "\n",
        "WordNet is a set of hierarchical relations between various nouns, adjectives, adverbs, and other parts of speech in the English language designed to capture semantic and lexical distance in the hierarchy. The main method of organization is the use of 'synsets', which are connected via higher/lower order relations (hypernym/hyponym), part/whole relations (meronym/holonym), or specificity/subset relations (troponym)."
      ],
      "metadata": {
        "id": "_d6n3RaGGlDe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import wordnet as wn\n",
        "bechSyn = wn.synsets('bechamel')\n",
        "print(bechSyn)\n",
        "\n",
        "print(bechSyn[0].definition())\n",
        "print(bechSyn[0].examples())\n",
        "print(bechSyn[0].lemmas(), \"\\n\")\n",
        "\n",
        "bechHyp = bechSyn[0].hypernyms()[0]\n",
        "entity = wn.synset('entity.n.01')\n",
        "while bechHyp:\n",
        "  print(bechHyp)\n",
        "  if bechHyp == entity:\n",
        "    break\n",
        "  if bechHyp.hypernyms():\n",
        "    bechHyp = bechHyp.hypernyms()[0]\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yj9eKOZBEMk3",
        "outputId": "d6e6675a-b2a7-4c66-e140-6c7286dec38f"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Synset('white_sauce.n.01')]\n",
            "milk thickened with a butter and flour roux\n",
            "[]\n",
            "[Lemma('white_sauce.n.01.white_sauce'), Lemma('white_sauce.n.01.bechamel_sauce'), Lemma('white_sauce.n.01.bechamel')] \n",
            "\n",
            "Synset('sauce.n.01')\n",
            "Synset('condiment.n.01')\n",
            "Synset('flavorer.n.01')\n",
            "Synset('ingredient.n.03')\n",
            "Synset('foodstuff.n.02')\n",
            "Synset('food.n.01')\n",
            "Synset('substance.n.07')\n",
            "Synset('matter.n.03')\n",
            "Synset('physical_entity.n.01')\n",
            "Synset('entity.n.01')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Regarding the hypernym structure, everything from 'entity' to 'food' seems like it follows a fairly straightforward logical path, but beyond those the order of some of the categories seems unclear- looking at foodstuff being a hyponym of food, what could be food that does not count as a foodstuff, and if there is none then why are there distinct levels? Some more apparently ambiguous relations that stick out are the condiment-sauce and ingredient-flavorer hypernym/hyponym relationships- it seems like one could come up with arguments for those relationships to go either way, and it is easy to imagine for some words there are even more ambiguous relations, so how are these structures finally decided? Overall though, the increase in specificity throughout the path displayed here seems like a reasonable balance between sufficiently compartmentalized without creating an overly complex network that would be too computationally expensive to parse."
      ],
      "metadata": {
        "id": "WazS-1UDZuYg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Hypernyms:\")\n",
        "print(bechSyn[0].hypernyms())\n",
        "print(\"Hyponyms:\")\n",
        "print(bechSyn[0].hyponyms())\n",
        "print(\"Meronyms:\")\n",
        "print(bechSyn[0].member_meronyms())\n",
        "print(\"Holonyms:\")\n",
        "print(bechSyn[0].member_holonyms())\n",
        "print(\"Antonyms\")\n",
        "print(bechSyn[0].lemmas()[0].antonyms())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cfhSagzlkarm",
        "outputId": "568999e0-9597-4eee-cc61-3069ff13a9dd"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hypernyms:\n",
            "[Synset('sauce.n.01')]\n",
            "Hyponyms:\n",
            "[Synset('blanc.n.01'), Synset('cheese_sauce.n.01'), Synset('cream_sauce.n.01')]\n",
            "Meronyms:\n",
            "[]\n",
            "Holonyms:\n",
            "[]\n",
            "Antonyms\n",
            "[]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "flaSyn = wn.synsets('flambe', pos=wn.VERB)\n",
        "print(flaSyn)\n",
        "\n",
        "print(flaSyn[0].definition())\n",
        "print(flaSyn[0].examples())\n",
        "print(flaSyn[0].lemmas(), \"\\n\")\n",
        "\n",
        "flaHyp = flaSyn[0].hypernyms()[0]\n",
        "entity = wn.synset('make.v.03')\n",
        "while flaHyp:\n",
        "  print(flaHyp)\n",
        "  if flaHyp == entity:\n",
        "    break\n",
        "  if flaHyp.hypernyms():\n",
        "    flaHyp = flaHyp.hypernyms()[0]"
      ],
      "metadata": {
        "id": "X7chrkmwPPdu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2d8ddfdf-818a-4ff9-f1f5-f0fd6efbe501"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Synset('flambe.v.01')]\n",
            "pour liquor over and ignite (a dish)\n",
            "[]\n",
            "[Lemma('flambe.v.01.flambe')] \n",
            "\n",
            "Synset('cook.v.02')\n",
            "Synset('create_from_raw_material.v.01')\n",
            "Synset('make.v.03')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The structure of verbs in WordNet appears to be more diverse, with multiple different options for the highest level hypernym. Before this word, I also tried 'juggle' to see if a more general verb would prompt a larger chain of hypernyms, and while it was larger than 'flambe' it was not nearly as long as the noun form of 'juggle'. It appears based off of these experiments that the structure of verbs in WordNet is less rigorous than with nouns. With the selected example in particular, I would have guessed that there would be at least one more hypernym between flambe and cook, but I assume for model complexity's sake that uncommon words are deprioritized."
      ],
      "metadata": {
        "id": "DzabdrP5JkNC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(wn.morphy('flambed', pos=wn.VERB))\n",
        "print(wn.morphy('flambe', pos=wn.ADJ))\n",
        "print(wn.morphy('flambe', pos=wn.NOUN))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3SB8BvUmUgjn",
        "outputId": "9ce575f0-faf9-4076-f4d6-d3511cb97b8b"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "flambe\n",
            "None\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Given such a specific word, morphy can only recover the original form given different tenses."
      ],
      "metadata": {
        "id": "Vyizin_nWFxM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def traceHypernyms(syn):\n",
        "  hyp = syn.hypernyms()[0]\n",
        "  entity = wn.synset('entity.n.01')\n",
        "  while hyp:\n",
        "    print(hyp)\n",
        "    if hyp == entity:\n",
        "      break\n",
        "    if hyp.hypernyms():\n",
        "      hyp = hyp.hypernyms()[0]"
      ],
      "metadata": {
        "id": "LjXe-oFdZmRd"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.wsd import lesk\n",
        "word1 = 'pilot'\n",
        "word2 = 'captain'\n",
        "\n",
        "syn1 = wn.synsets(word1, pos=wn.NOUN)\n",
        "syn2 = wn.synsets(word2, pos=wn.NOUN)\n",
        "similar1 = syn1[0]\n",
        "similar2 = syn2[3]\n",
        "\n",
        "print(similar1.definition())\n",
        "print(similar1.lemmas())\n",
        "traceHypernyms(similar1)\n",
        "print()\n",
        "print(similar2.definition())\n",
        "print(similar2.lemmas())\n",
        "traceHypernyms(similar2)\n",
        "\n",
        "print(\"\\nWu-Palmer Similarity: \", wn.wup_similarity(similar1, similar2), \"\\n\")\n",
        "sent = ['The', 'pilot', 'had', 'to', 'get', 'the', 'cargo', 'shipment', 'to', 'the', 'captain', 'on', 'time', '.']\n",
        "sent2 = ['The', 'airplane', 'pilot', 'had', 'to', 'get', 'the', 'cargo', 'shipment', 'to', 'the', 'captain', 'on', 'time', '.']\n",
        "sent3 = ['The', 'airplane', 'pilot', 'had', 'to', 'get', 'the', 'cargo', 'shipment', 'to', 'the', 'airport', 'for', 'the', 'captain', 'to', 'leave', 'the', 'dock', 'on', 'time', '.']\n",
        "\n",
        "lesk1 = lesk(sent3, 'pilot', 'n')\n",
        "lesk2 = lesk(sent3, 'captain', 'n')\n",
        "print(lesk1)\n",
        "print(lesk1.definition())\n",
        "print(lesk2)\n",
        "print(lesk2.definition())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bX76RusSXCZl",
        "outputId": "14320e23-fed5-43f2-a937-4dd6f5768511"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "someone who is licensed to operate an aircraft in flight\n",
            "[Lemma('pilot.n.01.pilot'), Lemma('pilot.n.01.airplane_pilot')]\n",
            "Synset('aviator.n.01')\n",
            "Synset('skilled_worker.n.01')\n",
            "Synset('worker.n.01')\n",
            "Synset('person.n.01')\n",
            "Synset('causal_agent.n.01')\n",
            "Synset('physical_entity.n.01')\n",
            "Synset('entity.n.01')\n",
            "\n",
            "an officer who is licensed to command a merchant ship\n",
            "[Lemma('master.n.07.master'), Lemma('master.n.07.captain'), Lemma('master.n.07.sea_captain'), Lemma('master.n.07.skipper')]\n",
            "Synset('officer.n.04')\n",
            "Synset('mariner.n.01')\n",
            "Synset('sailor.n.01')\n",
            "Synset('skilled_worker.n.01')\n",
            "Synset('worker.n.01')\n",
            "Synset('person.n.01')\n",
            "Synset('causal_agent.n.01')\n",
            "Synset('physical_entity.n.01')\n",
            "Synset('entity.n.01')\n",
            "\n",
            "Wu-Palmer Similarity:  0.5 \n",
            "\n",
            "Synset('fender.n.02')\n",
            "an inclined metal frame at the front of a locomotive to clear the track\n",
            "Synset('captain.n.06')\n",
            "the pilot in charge of an airship\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Wu-Palmer similarity score makes sense for the selected synsets, as they share a large portion of their upper hypernym path after observation of the manual printing. At first glance I would have thought that the score would be higher, but I suspect the extended length of captain's hypernym path is what is decreasing the score. The results of the lesk algorithm are more interesting however, as I selected the context sentence to be slightly ambiguous without being enigmatic. The first context sentence produced a not entirely nonsense definition for pilot, although it is quite a stretch especially considering the context decided on for captain. Curious if it would respond to additional help, I added a word to the sentence hoping that it would find the correct definition for pilot, but it still turned out the same, which I find baffling. The third sentence tries to give it as much help as possible, and yet the results do not change. The airship definition for the captain still makes sense in all three contexts given the suggestion of aerial transport, but I can't think of anything to indicate railway travel, especially in the last two sentences."
      ],
      "metadata": {
        "id": "ujMJAsN7jnhY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('sentiwordnet')\n",
        "from nltk.corpus import sentiwordnet as swn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-2TMCkIXo4-i",
        "outputId": "46e6c999-d327-4dd7-e25e-e8573a2fa305"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package sentiwordnet to /root/nltk_data...\n",
            "[nltk_data]   Package sentiwordnet is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##SentiWordNet, Briefly\n",
        "\n",
        "SentiWordNet is a sentiment analysis tool built on top of WordNet, leveraging the synset structure of WordNet to assign sentiment value scores for positiviy, negativity, and objectivity. It seems ideal for cases where one is already leveraging the power of WordNet in their analysis of a corpus to quickly get an idea of the tone of the sentences, where the ease of use would be a valid tradeoff against other more powerful tools."
      ],
      "metadata": {
        "id": "A7xUMmVYpaCp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "znrPX7cWmz4q",
        "outputId": "4502d3da-a8f3-419b-ac4e-5c6e7d2405fa"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chargeWord = 'disdain'\n",
        "neg = pos = 0\n",
        "chargeSyn = list(swn.senti_synsets(chargeWord))\n",
        "for syn in chargeSyn:\n",
        "  print(syn)\n",
        "  print(\"Positive score = \", syn.pos_score())\n",
        "  print(\"Negative score = \", syn.neg_score())\n",
        "  print(\"Objective score = \", syn.obj_score(), \"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HqClawjLpGN-",
        "outputId": "81f2704c-1313-4694-ce10-959336941e16"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<contempt.n.01: PosScore=0.0 NegScore=0.625>\n",
            "Positive score =  0.0\n",
            "Negative score =  0.625\n",
            "Objective score =  0.375 \n",
            "\n",
            "<condescension.n.02: PosScore=0.375 NegScore=0.0>\n",
            "Positive score =  0.375\n",
            "Negative score =  0.0\n",
            "Objective score =  0.625 \n",
            "\n",
            "<contemn.v.01: PosScore=0.0 NegScore=0.375>\n",
            "Positive score =  0.0\n",
            "Negative score =  0.375\n",
            "Objective score =  0.625 \n",
            "\n",
            "<reject.v.04: PosScore=0.0 NegScore=0.25>\n",
            "Positive score =  0.0\n",
            "Negative score =  0.25\n",
            "Objective score =  0.75 \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Regarding the analysis of a single word, the apparently high objectivity scores for the sentisynsets related to what to me is a highly negatively charged word, disdain, are surprising- even the most negatively rated related word is given 0.625. This makes me curious as to if these value assignments could be interpreted more as a probability of seeing the word in a given context rather than a scaled value of sentiment intensity. This seems doubtful looking at the score for condescention however- what possible context could that be used positively? Despite the a given individual word's seemingly dubious scoring, I imagine averaged over an entire document it could give at least a good enough idea of the tone of the text. I imagine how some kind of sentiment analysis could help guide a chatbot such as chatGPT to help it modify its output if it seems that the user is becoming frustrated with it or something along those lines."
      ],
      "metadata": {
        "id": "4m8Do_7OiXmB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentiSent = \"The crabby and overworked advisor tore the poor graduate student's thesis to shreds.\"\n",
        "sentiTok = word_tokenize(sentiSent)\n",
        "overPos = overNeg = overObj = count = 0\n",
        "for tok in sentiTok:\n",
        "  syn = list(swn.senti_synsets(tok))\n",
        "  if syn:\n",
        "    count +=1\n",
        "    syn = syn[0]\n",
        "    print(syn)\n",
        "    print(\"Positive score = \", syn.pos_score())\n",
        "    overPos += syn.pos_score()\n",
        "    print(\"Negative score = \", syn.neg_score())\n",
        "    overNeg += syn.neg_score()\n",
        "    print(\"Objective score = \", syn.obj_score(), \"\\n\")\n",
        "    overObj += syn.obj_score()\n",
        "print(\"Overall Rating of the Sentence: \\nPositive:\", overPos/count,\"Negative:\", overNeg/count, \"Objective:\", overObj/count)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ITBQZ-NrnA_U",
        "outputId": "02a4b93e-9963-448b-d654-b2c1e975adbd"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<crabbed.s.01: PosScore=0.0 NegScore=0.625>\n",
            "Positive score =  0.0\n",
            "Negative score =  0.625\n",
            "Objective score =  0.375 \n",
            "\n",
            "<overwork.v.01: PosScore=0.0 NegScore=0.0>\n",
            "Positive score =  0.0\n",
            "Negative score =  0.0\n",
            "Objective score =  1.0 \n",
            "\n",
            "<adviser.n.01: PosScore=0.0 NegScore=0.0>\n",
            "Positive score =  0.0\n",
            "Negative score =  0.0\n",
            "Objective score =  1.0 \n",
            "\n",
            "<torus.n.02: PosScore=0.0 NegScore=0.0>\n",
            "Positive score =  0.0\n",
            "Negative score =  0.0\n",
            "Objective score =  1.0 \n",
            "\n",
            "<poor_people.n.01: PosScore=0.0 NegScore=0.0>\n",
            "Positive score =  0.0\n",
            "Negative score =  0.0\n",
            "Objective score =  1.0 \n",
            "\n",
            "<alumnus.n.01: PosScore=0.0 NegScore=0.0>\n",
            "Positive score =  0.0\n",
            "Negative score =  0.0\n",
            "Objective score =  1.0 \n",
            "\n",
            "<student.n.01: PosScore=0.0 NegScore=0.0>\n",
            "Positive score =  0.0\n",
            "Negative score =  0.0\n",
            "Objective score =  1.0 \n",
            "\n",
            "<thesis.n.01: PosScore=0.0 NegScore=0.0>\n",
            "Positive score =  0.0\n",
            "Negative score =  0.0\n",
            "Objective score =  1.0 \n",
            "\n",
            "<shred.n.01: PosScore=0.125 NegScore=0.0>\n",
            "Positive score =  0.125\n",
            "Negative score =  0.0\n",
            "Objective score =  0.875 \n",
            "\n",
            "Overall Rating of the Sentence: \n",
            "Positive: 0.013888888888888888 Negative: 0.06944444444444445 Objective: 0.9166666666666666\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Regarding the performance on an entire sentence, the most immediately noticeable aspect is the discrepancies between the words and the sentisynset associated with it- because it judges sentiment based on individual tokens, it misses the collocation of 'graduate student', selecting alumnus for graduate. The most surprising mismatch is between the past tense of tear and the gemetrical shape of the torus; I would expect that it would have some way to determine based on some measure of word likelihood in an ambiguous case such as this, especially as 'tore' is an archaic term. As far as the sentiment ratings, it was at least able to detect that the sentence was more negative than positive, but it couldn't pick up on the figurative language in 'tore it to shreds', assigning an overall positive score if you average the score for all of those words. The sentence overall was ruled to be very highly objective, which I doubt any human reading it would agree with, highlighting the limitations of this simplistic approach."
      ],
      "metadata": {
        "id": "YE5SVTZ-wcOV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Collocations, Briefly\n",
        "\n",
        "Collocations are sets of two or more words that have a specific meaning when used together distinct from using the two words independently or substitutions of synonyms. Collocations can be uncovered by searching for pairs of words, or bigrams, that occur with a frequency higher than randomness would dictate, calculated via point-wise mutual information."
      ],
      "metadata": {
        "id": "ch6h2D4uqsGI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.collocations import *\n",
        "nltk.download('gutenberg')\n",
        "nltk.download('genesis')\n",
        "nltk.download('inaugural')\n",
        "nltk.download('nps_chat')\n",
        "nltk.download('webtext')\n",
        "nltk.download('treebank')\n",
        "from nltk.book import text4\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "id": "tCqbOYTJNmto",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0a4bc318-dcfa-4bde-e332-7128c85ee061"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]   Package gutenberg is already up-to-date!\n",
            "[nltk_data] Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]   Package genesis is already up-to-date!\n",
            "[nltk_data] Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]   Package inaugural is already up-to-date!\n",
            "[nltk_data] Downloading package nps_chat to /root/nltk_data...\n",
            "[nltk_data]   Package nps_chat is already up-to-date!\n",
            "[nltk_data] Downloading package webtext to /root/nltk_data...\n",
            "[nltk_data]   Package webtext is already up-to-date!\n",
            "[nltk_data] Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]   Package treebank is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "colloc = text4.collocations()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t5E13QhBfPUb",
        "outputId": "1b66d522-dbe1-4e46-cebc-fa6a0d3abde8"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "United States; fellow citizens; years ago; four years; Federal\n",
            "Government; General Government; American people; Vice President; God\n",
            "bless; Chief Justice; one another; fellow Americans; Old World;\n",
            "Almighty God; Fellow citizens; Chief Magistrate; every citizen; Indian\n",
            "tribes; public debt; foreign nations\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import itertools\n",
        "from nltk.collocations import BigramCollocationFinder\n",
        "from nltk.metrics import BigramAssocMeasures\n",
        "\n",
        "def getPotentialColloc(text, scoreFunct = BigramAssocMeasures.chi_sq, n=200):\n",
        "  finder = BigramCollocationFinder.from_words(text)\n",
        "  bigrams = finder.nbest(scoreFunct, n)\n",
        "  return dict([(ngram, True) for ngram in itertools.chain(text, bigrams) if type(ngram) == tuple])"
      ],
      "metadata": {
        "id": "TRkv3QR2uv9-"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text4colloc = getPotentialColloc(text4)\n",
        "print(text4colloc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yCqv85zXxK0L",
        "outputId": "a1ce2522-61bb-468f-b27c-b92084e03df5"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{('/', '11'): True, ('25', 'straight'): True, ('Amelia', 'Island'): True, ('Apollo', 'astronauts'): True, ('Archibald', 'MacLeish'): True, ('BUSINESS', 'COOPERATION'): True, ('Barbary', 'Powers'): True, ('Belleau', 'Wood'): True, ('Boston', 'lawyer'): True, ('Britannic', 'Majesty'): True, ('COOPERATION', 'BY'): True, ('CRIMINAL', 'JUSTICE'): True, ('Calvin', 'Coolidge'): True, ('Cape', 'Horn'): True, ('Cardinal', 'Bernardin'): True, ('Chop', 'Hill'): True, ('Chosin', 'Reservoir'): True, ('Christmas', 'Eve'): True, ('Colonel', 'Goethals'): True, ('Dark', 'pictures'): True, ('Domestic', 'Product'): True, ('EIGHTEENTH', 'AMENDMENT'): True, ('Emancipation', 'Proclamation'): True, ('English', 'writer'): True, ('Fort', 'Sumter'): True, ('Gatun', 'dam'): True, ('Golden', 'Rule'): True, ('Gross', 'Domestic'): True, ('Growing', 'connections'): True, ('Hague', 'Tribunal'): True, ('Herein', 'flows'): True, ('Holy', 'Writ'): True, ('Hope', 'maketh'): True, ('Information', 'Age'): True, ('Iwo', 'Jima'): True, ('Joseph', 'Warren'): True, ('Julia', 'Coleman'): True, ('Khe', 'Sahn'): True, ('Lady', 'Michelle'): True, ('MANDATES', 'FROM'): True, ('Magna', 'Charta'): True, ('Mayflower', 'Compact'): True, ('Miss', 'Julia'): True, ('NATIONAL', 'INVESTIGATION'): True, ('Naval', 'Commissioners'): True, ('November', '1963'): True, ('OTHER', 'MANDATES'): True, ('Omaha', 'Beach'): True, ('PARTY', 'RESPONSIBILITIES'): True, ('PUBLIC', 'HEALTH'): True, ('Panama', 'Canal'): True, ('Penetrating', 'internally'): True, ('Persistent', 'importunity'): True, ('Philippine', 'Islands'): True, ('Pork', 'Chop'): True, ('Porto', 'Rico'): True, ('Reflecting', 'Pool'): True, ('Representative', 'Gillis'): True, ('Rio', 'de'): True, ('Rocky', 'Mountains'): True, ('SPECIAL', 'SESSION'): True, ('Saint', 'Augustine'): True, ('Santo', 'Domingo'): True, ('Seneca', 'Falls'): True, ('Superficial', 'observers'): True, ('TO', 'BUSINESS'): True, ('Valley', 'Forge'): True, ('WORLD', 'PEACE'): True, ('Winston', 'Churchill'): True, ('Xthe', 'unfinished'): True, ('adventurously', 'ascended'): True, ('agonizing', 'spasms'): True, ('approximate', 'disarmament'): True, ('astronauts', 'flew'): True, ('bona', 'fide'): True, ('broadcast', 'instantaneously'): True, ('bulky', 'merchandise'): True, ('casual', 'observer'): True, ('cheap', 'routes'): True, ('childhood', 'rocked'): True, ('clarified', 'atmosphere'): True, ('concise', 'enumeration'): True, ('convenient', 'coaling'): True, ('conventional', 'stipulations'): True, ('cordially', 'concur'): True, ('crippling', 'drought'): True, ('de', 'Janeiro'): True, ('defamatory', 'publications'): True, ('delicious', 'recollection'): True, ('delusive', 'speculations'): True, ('densely', 'populated'): True, ('deplorably', 'saps'): True, ('disfranchised', 'peasantry'): True, ('domiciliary', 'vexation'): True, ('efficiently', 'performs'): True, ('elder', 'Brutus'): True, ('elementary', 'decencies'): True, ('embittered', 'travail'): True, ('entailing', 'idleness'): True, ('errant', 'humors'): True, ('excursions', 'whither'): True, ('explicitly', 'parted'): True, ('exterminating', 'havoc'): True, ('famed', 'Rainbow'): True, ('fondest', 'predilection'): True, ('funeral', 'pile'): True, ('green', 'slopes'): True, ('habeas', 'corpus'): True, ('impassable', 'wall'): True, ('imperishable', 'glories'): True, ('impracticable', 'withal'): True, ('indignantly', 'frowning'): True, ('instantaneously', 'tobillions'): True, ('insurmountable', 'arose'): True, ('intensely', 'modem'): True, ('jailed', 'dissidents'): True, ('joyful', 'mountaintop'): True, ('keen', 'rivalry'): True, ('laissez', 'faire'): True, ('legalized', 'larceny'): True, ('lifeless', 'tree'): True, ('lightly', 'burthened'): True, ('literary', 'altercation'): True, ('luring', 'fallacy'): True, ('measureless', 'wastage'): True, ('mystic', 'chords'): True, ('narrowly', 'nationalistic'): True, ('needless', 'additions'): True, ('nourish', 'starved'): True, ('obsessions', 'cripple'): True, ('outlaw', 'regimes'): True, ('paraphrase', 'Winston'): True, ('pitiable', 'slackerism'): True, ('plainest', 'implication'): True, ('poet', 'Archibald'): True, ('preoccupations', 'absorbing'): True, ('pres', 'urgently'): True, ('presaging', 'flashes'): True, ('prohibitory', 'clauses'): True, ('prudential', 'economies'): True, ('publicly', 'repeating'): True, ('purposeful', 'rekindling'): True, ('readjusts', 'compensations'): True, ('regional', 'groupings'): True, ('reverend', 'clergy'): True, ('rolling', 'mills'): True, ('ruthlessly', 'breaks'): True, ('sad', 'depletion'): True, ('scheming', 'politician'): True, ('settler', 'pushes'): True, ('shameful', 'confession'): True, ('shamefully', 'prodigal'): True, ('sharp', 'thrust'): True, ('shorten', 'distances'): True, ('shorter', 'waterway'): True, ('silently', 'stalks'): True, ('singly', 'cope'): True, ('skillful', 'artisans'): True, ('slaughtering', 'innocents'): True, ('surviving', 'warriors'): True, ('tangible', 'determinations'): True, ('tiny', 'fraction'): True, ('tropic', 'suns'): True, ('unbounded', 'hospitality'): True, ('unceasingly', 'preyed'): True, ('uncrossed', 'desert'): True, ('uniformly', 'outrun'): True, ('unitedly', 'marshals'): True, ('untried', 'cares'): True, ('unwholesome', 'progeny'): True, ('watchman', 'waketh'): True, ('watercourses', 'undeveloped'): True, ('wily', 'craft'): True, ('wings', '\";'): True, ('¡¨¡', 'Xthey'): True, (\"'\", 's'): True, ('Founding', 'Fathers'): True, ('Social', 'Security'): True, ('Indian', 'tribes'): True, ('Abraham', 'Lincoln'): True, ('specie', 'payments'): True, ('illegal', 'liquor'): True, ('merchant', 'marine'): True, ('Western', 'Hemisphere'): True, ('Old', 'World'): True, ('9', '/'): True, ('Assessing', 'realistically'): True, ('Bicentennial', 'Inauguration'): True, ('CONGRESS', 'Action'): True, ('Congressman', 'Michael'): True, ('Delano', 'Roosevelt'): True, ('Discouraging', 'indebtedness'): True, ('Disease', 'diminishes'): True, ('Franklin', 'Delano'): True, ('GOVERNMENT', 'TO'): True, ('Gettysburg', 'Address'): True, ('Gillis', 'Long'): True, ('Humanity', 'hungers'): True, ('John', 'Page'): True, ('John', 'Stennis'): True}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "t4citer = list(text4colloc)\n",
        "t4colloc = list()\n",
        "\n",
        "vocab = len(set(text4))\n",
        "\n",
        "text4t = ' '.join(text4.tokens)\n",
        "\n",
        "for bigram in t4citer:\n",
        "  if(bigram[0].isalpha() and bigram[1].isalpha):\n",
        "    w1 = bigram[0]\n",
        "    w2 = bigram[1]\n",
        "    #print(w1, w2)\n",
        "    bigr = text4t.count(w1 + ' ' + w2)/vocab\n",
        "    w1c = text4t.count(w1)/vocab\n",
        "    w2c = text4t.count(w2)/vocab\n",
        "    #print(bigr, w1c, w2c)\n",
        "    if(bigr != 0 and w1c != 0 and w2c != 0):\n",
        "      pmi = math.log2(bigr/(w1c*w2c))\n",
        "      #print(w1, w2, pmi)\n",
        "      if(pmi > 0):\n",
        "        t4colloc.append([w1 + ' ' + w2, pmi])\n",
        "\n",
        "for colloc in t4colloc:\n",
        "  print(colloc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wtlcPnTGyx5p",
        "outputId": "8d8f120d-48b8-49ed-d55b-20d80c98b60b"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Amelia Island', 11.706352115508489]\n",
            "['Apollo astronauts', 13.291314616229645]\n",
            "['Archibald MacLeish', 13.291314616229645]\n",
            "['BUSINESS COOPERATION', 13.291314616229645]\n",
            "['Barbary Powers', 13.291314616229645]\n",
            "['Belleau Wood', 13.291314616229645]\n",
            "['Boston lawyer', 13.291314616229645]\n",
            "['Britannic Majesty', 13.291314616229645]\n",
            "['COOPERATION BY', 13.291314616229645]\n",
            "['CRIMINAL JUSTICE', 13.291314616229645]\n",
            "['Calvin Coolidge', 13.291314616229645]\n",
            "['Cape Horn', 13.291314616229645]\n",
            "['Cardinal Bernardin', 13.291314616229645]\n",
            "['Chop Hill', 13.291314616229645]\n",
            "['Chosin Reservoir', 13.291314616229645]\n",
            "['Christmas Eve', 8.384424020621127]\n",
            "['Colonel Goethals', 13.291314616229645]\n",
            "['Dark pictures', 13.291314616229645]\n",
            "['Domestic Product', 13.291314616229645]\n",
            "['EIGHTEENTH AMENDMENT', 13.291314616229645]\n",
            "['Emancipation Proclamation', 13.291314616229645]\n",
            "['English writer', 11.706352115508489]\n",
            "['Fort Sumter', 10.483959694172041]\n",
            "['Gatun dam', 9.04338710278606]\n",
            "['Golden Rule', 11.706352115508489]\n",
            "['Gross Domestic', 13.291314616229645]\n",
            "['Growing connections', 13.291314616229645]\n",
            "['Hague Tribunal', 13.291314616229645]\n",
            "['Herein flows', 13.291314616229645]\n",
            "['Holy Writ', 13.291314616229645]\n",
            "['Hope maketh', 13.291314616229645]\n",
            "['Information Age', 12.291314616229645]\n",
            "['Iwo Jima', 13.291314616229645]\n",
            "['Joseph Warren', 13.291314616229645]\n",
            "['Julia Coleman', 13.291314616229645]\n",
            "['Khe Sahn', 13.291314616229645]\n",
            "['Lady Michelle', 13.291314616229645]\n",
            "['MANDATES FROM', 13.291314616229645]\n",
            "['Magna Charta', 13.291314616229645]\n",
            "['Mayflower Compact', 13.291314616229645]\n",
            "['Miss Julia', 10.121389614787333]\n",
            "['NATIONAL INVESTIGATION', 13.291314616229645]\n",
            "['Naval Commissioners', 13.291314616229645]\n",
            "['November 1963', 13.291314616229645]\n",
            "['OTHER MANDATES', 13.291314616229645]\n",
            "['Omaha Beach', 13.291314616229645]\n",
            "['PARTY RESPONSIBILITIES', 13.291314616229645]\n",
            "['PUBLIC HEALTH', 13.291314616229645]\n",
            "['Panama Canal', 12.291314616229645]\n",
            "['Penetrating internally', 13.291314616229645]\n",
            "['Persistent importunity', 13.291314616229645]\n",
            "['Philippine Islands', 10.291314616229645]\n",
            "['Pork Chop', 13.291314616229645]\n",
            "['Porto Rico', 12.291314616229645]\n",
            "['Reflecting Pool', 13.291314616229645]\n",
            "['Representative Gillis', 10.706352115508489]\n",
            "['Rio de', 1.3143925483785655]\n",
            "['Rocky Mountains', 12.291314616229645]\n",
            "['SPECIAL SESSION', 13.291314616229645]\n",
            "['Saint Augustine', 13.291314616229645]\n",
            "['Santo Domingo', 11.706352115508489]\n",
            "['Seneca Falls', 13.291314616229645]\n",
            "['Superficial observers', 13.291314616229645]\n",
            "['TO BUSINESS', 13.291314616229645]\n",
            "['Valley Forge', 13.291314616229645]\n",
            "['WORLD PEACE', 13.291314616229645]\n",
            "['Winston Churchill', 13.291314616229645]\n",
            "['Xthe unfinished', 12.291314616229645]\n",
            "['adventurously ascended', 13.291314616229645]\n",
            "['agonizing spasms', 13.291314616229645]\n",
            "['approximate disarmament', 12.291314616229645]\n",
            "['astronauts flew', 13.291314616229645]\n",
            "['bona fide', 6.291314616229645]\n",
            "['broadcast instantaneously', 13.291314616229645]\n",
            "['bulky merchandise', 13.291314616229645]\n",
            "['casual observer', 11.291314616229645]\n",
            "['cheap routes', 12.291314616229645]\n",
            "['childhood rocked', 13.291314616229645]\n",
            "['clarified atmosphere', 13.291314616229645]\n",
            "['concise enumeration', 13.291314616229645]\n",
            "['convenient coaling', 11.706352115508489]\n",
            "['conventional stipulations', 13.291314616229645]\n",
            "['cordially concur', 10.969386521342283]\n",
            "['crippling drought', 13.291314616229645]\n",
            "['de Janeiro', 1.3143925483785655]\n",
            "['defamatory publications', 13.291314616229645]\n",
            "['delicious recollection', 13.291314616229645]\n",
            "['delusive speculations', 13.291314616229645]\n",
            "['densely populated', 13.291314616229645]\n",
            "['deplorably saps', 13.291314616229645]\n",
            "['disfranchised peasantry', 13.291314616229645]\n",
            "['domiciliary vexation', 13.291314616229645]\n",
            "['efficiently performs', 12.291314616229645]\n",
            "['elder Brutus', 11.706352115508489]\n",
            "['elementary decencies', 13.291314616229645]\n",
            "['embittered travail', 13.291314616229645]\n",
            "['entailing idleness', 13.291314616229645]\n",
            "['errant humors', 13.291314616229645]\n",
            "['excursions whither', 13.291314616229645]\n",
            "['explicitly parted', 10.969386521342283]\n",
            "['exterminating havoc', 13.291314616229645]\n",
            "['famed Rainbow', 13.291314616229645]\n",
            "['fondest predilection', 13.291314616229645]\n",
            "['funeral pile', 11.706352115508489]\n",
            "['green slopes', 13.291314616229645]\n",
            "['habeas corpus', 13.291314616229645]\n",
            "['impassable wall', 10.969386521342283]\n",
            "['imperishable glories', 13.291314616229645]\n",
            "['impracticable withal', 13.291314616229645]\n",
            "['indignantly frowning', 13.291314616229645]\n",
            "['instantaneously tobillions', 13.291314616229645]\n",
            "['insurmountable arose', 13.291314616229645]\n",
            "['intensely modem', 13.291314616229645]\n",
            "['jailed dissidents', 13.291314616229645]\n",
            "['joyful mountaintop', 13.291314616229645]\n",
            "['keen rivalry', 13.291314616229645]\n",
            "['laissez faire', 11.706352115508489]\n",
            "['legalized larceny', 13.291314616229645]\n",
            "['lifeless tree', 10.483959694172041]\n",
            "['lightly burthened', 12.291314616229645]\n",
            "['literary altercation', 13.291314616229645]\n",
            "['luring fallacy', 11.291314616229645]\n",
            "['measureless wastage', 13.291314616229645]\n",
            "['mystic chords', 11.706352115508489]\n",
            "['narrowly nationalistic', 13.291314616229645]\n",
            "['needless additions', 13.291314616229645]\n",
            "['nourish starved', 11.706352115508489]\n",
            "['obsessions cripple', 13.291314616229645]\n",
            "['outlaw regimes', 11.291314616229645]\n",
            "['paraphrase Winston', 13.291314616229645]\n",
            "['pitiable slackerism', 13.291314616229645]\n",
            "['plainest implication', 13.291314616229645]\n",
            "['poet Archibald', 12.291314616229645]\n",
            "['preoccupations absorbing', 13.291314616229645]\n",
            "['pres urgently', 4.277294145914711]\n",
            "['presaging flashes', 13.291314616229645]\n",
            "['prohibitory clauses', 13.291314616229645]\n",
            "['prudential economies', 13.291314616229645]\n",
            "['publicly repeating', 13.291314616229645]\n",
            "['purposeful rekindling', 13.291314616229645]\n",
            "['readjusts compensations', 13.291314616229645]\n",
            "['regional groupings', 13.291314616229645]\n",
            "['reverend clergy', 12.291314616229645]\n",
            "['rolling mills', 10.483959694172041]\n",
            "['ruthlessly breaks', 13.291314616229645]\n",
            "['sad depletion', 11.706352115508489]\n",
            "['scheming politician', 12.291314616229645]\n",
            "['settler pushes', 11.291314616229645]\n",
            "['shameful confession', 12.291314616229645]\n",
            "['shamefully prodigal', 12.291314616229645]\n",
            "['sharp thrust', 12.291314616229645]\n",
            "['shorten distances', 12.291314616229645]\n",
            "['shorter waterway', 10.969386521342283]\n",
            "['silently stalks', 13.291314616229645]\n",
            "['singly cope', 8.384424020621127]\n",
            "['skillful artisans', 10.291314616229645]\n",
            "['slaughtering innocents', 13.291314616229645]\n",
            "['surviving warriors', 13.291314616229645]\n",
            "['tangible determinations', 12.291314616229645]\n",
            "['tiny fraction', 7.081861250600696]\n",
            "['tropic suns', 11.706352115508489]\n",
            "['unbounded hospitality', 13.291314616229645]\n",
            "['unceasingly preyed', 13.291314616229645]\n",
            "['uncrossed desert', 10.706352115508489]\n",
            "['uniformly outrun', 12.291314616229645]\n",
            "['unitedly marshals', 13.291314616229645]\n",
            "['untried cares', 13.291314616229645]\n",
            "['unwholesome progeny', 13.291314616229645]\n",
            "['watchman waketh', 13.291314616229645]\n",
            "['watercourses undeveloped', 13.291314616229645]\n",
            "['wily craft', 12.291314616229645]\n",
            "['wings \";', 13.291314616229645]\n",
            "['Founding Fathers', 11.291314616229645]\n",
            "['Social Security', 11.291314616229645]\n",
            "['Indian tribes', 9.831882997592349]\n",
            "['Abraham Lincoln', 10.706352115508489]\n",
            "['specie payments', 10.121389614787333]\n",
            "['illegal liquor', 10.969386521342283]\n",
            "['merchant marine', 10.068922194893197]\n",
            "['Western Hemisphere', 10.02828021039585]\n",
            "['Old World', 8.983886091037398]\n",
            "['Assessing realistically', 12.291314616229645]\n",
            "['Bicentennial Inauguration', 12.291314616229645]\n",
            "['CONGRESS Action', 12.291314616229645]\n",
            "['Congressman Michael', 12.291314616229645]\n",
            "['Delano Roosevelt', 12.291314616229645]\n",
            "['Discouraging indebtedness', 12.291314616229645]\n",
            "['Disease diminishes', 12.291314616229645]\n",
            "['Franklin Delano', 12.291314616229645]\n",
            "['GOVERNMENT TO', 12.291314616229645]\n",
            "['Gettysburg Address', 12.291314616229645]\n",
            "['Gillis Long', 12.291314616229645]\n",
            "['Humanity hungers', 12.291314616229645]\n",
            "['John Page', 11.291314616229645]\n",
            "['John Stennis', 11.291314616229645]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "From skimming the output of the collocation finder, it appears to cast a wide net, grabbing many phrases that do not fit the category of a collocation- many names and phrases that could easily be substituted with synonyms. I suspect that this could be an issue with the somewhat restricted range of language that comes with the formal nature of an inaugral address violating a prior assumption made when deriving the point-wise mutual information formula, but it could also be simply due to the relatively simplistic nature of the technique.\n"
      ],
      "metadata": {
        "id": "GUQTIbyQ6ppg"
      }
    }
  ]
}